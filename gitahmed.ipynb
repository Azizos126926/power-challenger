{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "deb22f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a9770e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pd.read_csv(\"aggday.csv\" )\n",
    "filtered_df  ['data_user']= filtered_df  ['data_user'].astype(str)\n",
    "filtered_df  ['Source'] =filtered_df  ['consumer_device'] + '_'+'data_user'+'_'+filtered_df  ['data_user']\n",
    "# Convertir 'Date' en datetime\n",
    "filtered_df[\"Date\"] = pd.to_datetime(filtered_df[\"Date\"])\n",
    "# Créer toutes les combinaisons possibles entre Source et Date\n",
    "#sources = filtered_df[\"Source\"].unique()\n",
    "#date_range = pd.date_range(start=filtered_df[\"Date\"].min(), end=filtered_df[\"Date\"].max())\n",
    "# MultiIndex avec toutes les combinaisons Source x Date\n",
    "#full_index = pd.MultiIndex.from_product([sources, date_range], names=[\"Source\", \"Date\"])\n",
    "#full_df = pd.DataFrame(index=full_index).reset_index()\n",
    "# Fusion avec le dataset original\n",
    "#filtered_df = full_df.merge(filtered_df, on=[\"Source\", \"Date\"], how=\"left\")\n",
    "## Remplacer les valeurs NaN par 0\n",
    "#filtered_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6f5006bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHANGE NAME OF FOLDER AND FILES TO CORRESPOND ####\n",
    "climate_df = pd.read_excel(r\"C:\\Users\\User\\Desktop\\ZINDI\\Climate Data\\Kalam Climate Data.xlsx\")# Convert to datetime\n",
    "filtered_df[\"Date\"] = pd.to_datetime(filtered_df [\"Date\"])\n",
    "climate_df[\"Date Time\"] = pd.to_datetime(climate_df[\"Date Time\"])\n",
    "\n",
    "# Aggregate climate data to daily level\n",
    "climate_daily = climate_df.groupby(climate_df[\"Date Time\"].dt.date).agg({\n",
    "    \"Temperature (°C)\": \"mean\",\n",
    "    \"Dewpoint Temperature (°C)\": \"mean\",\n",
    "    \"U Wind Component (m/s)\": \"mean\",\n",
    "    \"V Wind Component (m/s)\": \"mean\",\n",
    "    \"Total Precipitation (mm)\": \"sum\",\n",
    "    \"Snowfall (mm)\": \"sum\",\n",
    "    \"Snow Cover (%)\": \"mean\",\n",
    "}).reset_index()\n",
    "\n",
    "# Convert 'Date' column in climate_daily to datetime format\n",
    "climate_daily.rename(columns={\"Date Time\": \"Date\"}, inplace=True)\n",
    "climate_daily[\"Date\"] = pd.to_datetime(climate_daily[\"Date\"])  # Ensure datetime64[ns]\n",
    "\n",
    "# Merge with complete_data\n",
    "complete_data = filtered_df .merge(climate_daily, on=\"Date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5f536c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' to datetime format if not already\n",
    "complete_data[\"Date\"] = pd.to_datetime(complete_data[\"Date\"])\n",
    "\n",
    "# Extraire les composantes temporelles\n",
    "complete_data[\"year\"] = complete_data[\"Date\"].dt.year  # Gardé tel quel\n",
    "complete_data[\"is_weekend\"] = (complete_data[\"Date\"].dt.dayofweek >= 5).astype(int)  # 1 si Sam/Dim, sinon 0\n",
    "\n",
    "# Transformation cyclique avec cosinus et sinus\n",
    "complete_data[\"month_cos\"] = np.cos(2 * np.pi * complete_data[\"Date\"].dt.month / 12)\n",
    "complete_data[\"week_cos\"] = np.cos(2 * np.pi * complete_data[\"Date\"].dt.isocalendar().week / 52)\n",
    "complete_data[\"day_cos\"] = np.cos(2 * np.pi * complete_data[\"Date\"].dt.day / 31)\n",
    "complete_data[\"day_of_week_cos\"] = np.cos(2 * np.pi * complete_data[\"Date\"].dt.dayofweek / 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2eadda41",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data[['consumer_device', 'data_user']] = complete_data['Source'].str.extract(r'(consumer_device_\\d+)_data_user_(\\d+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f1f10",
   "metadata": {},
   "source": [
    "# clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a789bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=complete_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cf97463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=complete_data  [['Source', 'Date', 'consumer_device',\n",
    "        'Temperature (°C)', \n",
    "       'Dewpoint Temperature (°C)', 'U Wind Component (m/s)',\n",
    "       'V Wind Component (m/s)', 'Total Precipitation (mm)', 'Snowfall (mm)',\n",
    "       'Snow Cover (%)', 'year', 'is_weekend', 'month_cos', 'week_cos',\n",
    "       'day_cos', 'day_of_week_cos' ,'kwh']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6840a0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Source  Cluster\n",
      "0  consumer_device_1_data_user_1        1\n",
      "1  consumer_device_1_data_user_1        1\n",
      "2  consumer_device_1_data_user_1        1\n",
      "3  consumer_device_1_data_user_1        1\n",
      "4  consumer_device_1_data_user_1        1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Sélectionner les colonnes pertinentes pour le clustering\n",
    "columns_for_clustering = ['v_red', 'power_factor', 'v_blue', 'v_yellow', 'kwh']\n",
    "X = df2[columns_for_clustering]\n",
    "\n",
    "# Normalisation des données\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Appliquer K-means clustering\n",
    "kmeans = KMeans(n_clusters=5)  # On peut ajuster le nombre de clusters\n",
    "df2['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Extraire les sources uniques\n",
    "unique_sources = df2['Source'].unique()\n",
    "\n",
    "# Créer un DataFrame avec les points uniques des sources\n",
    "df_unique_sources = df2[df2['Source'].isin(unique_sources)]\n",
    "\n",
    "# Afficher les résultats\n",
    "print(df_unique_sources[['Source', 'Cluster']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ba6e7d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davies-Bouldin Index: 0.7358122009905308\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "db_score = davies_bouldin_score(X_scaled, df2['Cluster'])\n",
    "print(f\"Davies-Bouldin Index: {db_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a1fd2156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14940\\2738764802.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cluster['Date'] = pd.to_datetime(df_cluster['Date'])\n"
     ]
    }
   ],
   "source": [
    "# Sélectionner la colonne 'Cluster' de df_unique_sources\n",
    "df_cluster = df_unique_sources[['Source', 'Date', 'Cluster']]\n",
    "\n",
    "# Convertir la colonne 'Date' des deux DataFrames au même format pour assurer une bonne fusion\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "df_cluster['Date'] = pd.to_datetime(df_cluster['Date'])\n",
    "\n",
    "# Fusionner 'data' et 'df_cluster' selon 'Source' et 'Date'\n",
    "merged_data = pd.merge(data, df_cluster, on=['Source', 'Date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "05703b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=merged_data.copy()\n",
    "data['Cluster']= data  ['Cluster'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf6cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b94e2e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pd.read_csv(\"Data/SampleSubmission.csv\")\n",
    "ss[['date', 'consumer_device', 'data_user']] = ss['ID'].str.extract(r'(\\d{4}-\\d{2}-\\d{2})_(consumer_device_\\d+)_data_user_(\\d+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4e007315",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss['Date']=ss['date']\n",
    "# Convert 'Date' to datetime format if not already\n",
    "ss[\"Date\"] = pd.to_datetime(ss[\"Date\"])\n",
    "\n",
    "# Extraire les composantes temporelles\n",
    "ss[\"year\"] = ss[\"Date\"].dt.year  # Gardé tel quel\n",
    "ss[\"is_weekend\"] = (ss[\"Date\"].dt.dayofweek >= 5).astype(int)  # 1 si Sam/Dim, sinon 0\n",
    "\n",
    "# Transformation cyclique avec cosinus et sinus\n",
    "ss[\"month_cos\"] = np.cos(2 * np.pi * ss[\"Date\"].dt.month / 12)\n",
    "ss[\"week_cos\"] = np.cos(2 * np.pi * ss[\"Date\"].dt.isocalendar().week / 52)\n",
    "ss[\"day_cos\"] = np.cos(2 * np.pi * ss[\"Date\"].dt.day / 31)\n",
    "ss[\"day_of_week_cos\"] = np.cos(2 * np.pi * ss[\"Date\"].dt.dayofweek / 7)\n",
    "### CHANGE NAME OF FOLDER AND FILES TO CORRESPOND ####\n",
    "climate_df = pd.read_excel(r\"C:\\Users\\User\\Desktop\\ZINDI\\Climate Data\\Kalam Climate Data.xlsx\")# Convert to datetime\n",
    "ss[\"Date\"] = pd.to_datetime(filtered_df [\"Date\"])\n",
    "climate_df[\"Date Time\"] = pd.to_datetime(climate_df[\"Date Time\"])\n",
    "\n",
    "# Aggregate climate data to daily level\n",
    "climate_daily = climate_df.groupby(climate_df[\"Date Time\"].dt.date).agg({\n",
    "    \"Temperature (°C)\": \"mean\",\n",
    "    \"Dewpoint Temperature (°C)\": \"mean\",\n",
    "    \"U Wind Component (m/s)\": \"mean\",\n",
    "    \"V Wind Component (m/s)\": \"mean\",\n",
    "    \"Total Precipitation (mm)\": \"sum\",\n",
    "    \"Snowfall (mm)\": \"sum\",\n",
    "    \"Snow Cover (%)\": \"mean\",\n",
    "}).reset_index()\n",
    "\n",
    "# Convert 'Date' column in climate_daily to datetime format\n",
    "climate_daily.rename(columns={\"Date Time\": \"Date\"}, inplace=True)\n",
    "climate_daily[\"Date\"] = pd.to_datetime(climate_daily[\"Date\"])  # Ensure datetime64[ns]\n",
    "\n",
    "# Merge with complete_data\n",
    "ss = ss.merge(climate_daily, on=\"Date\", how=\"left\")\n",
    "ss ['data_user']= ss ['data_user'].astype(str)\n",
    "ss ['Source'] =ss  ['consumer_device'] + '_'+'data_user'+'_'+ss  ['data_user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f60c03df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.drop(columns=[\"ID\",'date','data_user','kwh'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2e60280f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14940\\405763222.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cluster['Date'] = pd.to_datetime(df_cluster['Date'])\n"
     ]
    }
   ],
   "source": [
    "# Sélectionner la colonne 'Cluster' de df_unique_sources\n",
    "df_cluster = df_unique_sources[['Source', 'Date', 'Cluster']]\n",
    "\n",
    "# Convertir la colonne 'Date' des deux DataFrames au même format pour assurer une bonne fusion\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "df_cluster['Date'] = pd.to_datetime(df_cluster['Date'])\n",
    "\n",
    "# Fusionner 'data' et 'df_cluster' selon 'Source' et 'Date'\n",
    "ss = pd.merge(ss, df_cluster, on=['Source', 'Date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "618fcdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([data, ss], axis=0).reset_index(drop=True)\n",
    "data=result.iloc[:198832-6014,:]\n",
    "ss=result.iloc[-6014:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666396c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b2635c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 2.2837270\ttest: 10.7021054\tbest: 10.7021054 (0)\ttotal: 67ms\tremaining: 1m 6s\n",
      "200:\tlearn: 1.4470767\ttest: 7.3446261\tbest: 7.3446261 (200)\ttotal: 10.5s\tremaining: 41.8s\n",
      "400:\tlearn: 1.3754894\ttest: 6.8120153\tbest: 6.8120153 (400)\ttotal: 21.4s\tremaining: 32s\n",
      "600:\tlearn: 1.3492992\ttest: 6.7329202\tbest: 6.7329202 (600)\ttotal: 33.9s\tremaining: 22.5s\n",
      "800:\tlearn: 1.3326039\ttest: 6.6936875\tbest: 6.6932853 (798)\ttotal: 46.6s\tremaining: 11.6s\n",
      "999:\tlearn: 1.3165294\ttest: 6.6596787\tbest: 6.6596044 (998)\ttotal: 59.2s\tremaining: 0us\n",
      "\n",
      "bestTest = 6.65960442\n",
      "bestIteration = 998\n",
      "\n",
      "Shrink model to first 999 iterations.\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assurer que 'Date' est au format datetime\n",
    "data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "\n",
    "# Trier par Source puis par Date\n",
    "data = data.sort_values([\"Source\", \"Date\"])\n",
    "\n",
    "# Sélectionner les 30 derniers jours de chaque Source\n",
    "test_indices = data.groupby(\"Source\")[\"Date\"].transform(lambda x: x >= x.max() - pd.Timedelta(days=30))\n",
    "train_data = data[~test_indices]\n",
    "test_data = data[test_indices]\n",
    "# Sélectionner les 30 derniers jours de chaque Source\n",
    "#test_indices = data.groupby(\"Source\")[\"Date\"].transform(lambda x: x >= x.max() - pd.Timedelta(days=0))\n",
    "#train_data = data[~test_indices]\n",
    "#test_data = data[test_indices]\n",
    "\n",
    "# Séparer les features et la cible\n",
    "X_train, y_train = train_data.drop(columns=[\"kwh\", \"Date\"]), train_data[\"kwh\"]\n",
    "X_test, y_test = test_data.drop(columns=[\"kwh\", \"Date\"]), test_data[\"kwh\"]\n",
    "\n",
    "# Définir les colonnes catégorielles\n",
    "cat_features =  [\"consumer_device\", \"Source\", \"Cluster\"] \n",
    "\n",
    "# Initialiser et entraîner CatBoost\n",
    "model = CatBoostRegressor(iterations=1000, learning_rate=0.01, cat_features=cat_features, verbose=200)\n",
    "model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50)\n",
    "\n",
    "# Prédictions\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec15b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ss= ss.drop(columns=[\"Date\",'kwh'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "47603ae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_predss = model.predict(X_ss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3fda1873",
   "metadata": {},
   "outputs": [],
   "source": [
    "rer = pd.DataFrame(y_predss)\n",
    "rer.rename(columns={0: 'kwh'}, inplace=True)\n",
    "\n",
    "te = pd.read_csv(\"Data/SampleSubmission.csv\")\n",
    "te['kwh'] = rer['kwh']\n",
    "te.to_csv('subcat666.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
